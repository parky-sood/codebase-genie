{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parky-sood/codebase-genie/blob/main/RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vAcN9iufjX"
      },
      "source": [
        "# RAG Process Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSQbb-WI0Nb2"
      },
      "source": [
        "![Screenshot 2024-11-25 at 7 12 58 PM](https://github.com/user-attachments/assets/0bd67cf0-43d5-46d2-879c-a752cae4c8e3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPopS_r5u1h3"
      },
      "source": [
        "# Library Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj1jwDO4uwNw",
        "outputId": "350a23c5-1b9c-457a-bc48-6886a66b30a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygithub\n",
            "  Downloading PyGithub-2.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting pynacl>=1.4.0 (from pygithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from pygithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pygithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pygithub) (2.2.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pygithub) (1.2.15)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->pygithub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->pygithub) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pygithub) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading PyGithub-2.5.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.9/375.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_pinecone-0.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, pinecone-plugin-interface, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pynacl, pinecone-plugin-inference, pydantic-settings, pinecone-client, dataclasses-json, aiohttp, pygithub, langchain-core, langchain_pinecone, langchain, langchain-community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.2\n",
            "    Uninstalling aiohttp-3.11.2:\n",
            "      Successfully uninstalled aiohttp-3.11.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "Successfully installed SQLAlchemy-2.0.35 aiohttp-3.9.5 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.9 langchain-community-0.3.8 langchain-core-0.3.21 langchain_pinecone-0.2.0 marshmallow-3.23.1 mypy-extensions-1.0.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.6.1 pygithub-2.5.0 pynacl-1.5.0 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pygithub langchain langchain-community openai tiktoken pinecone-client langchain_pinecone sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUGhSItAvI3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fc1b29-d375-4ac0-8762-3de2b3959bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3AhzI37vA23"
      },
      "source": [
        "# Clone Repo Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z7Bj4WAvEJA"
      },
      "outputs": [],
      "source": [
        "def clone_repo(repo_url):\n",
        "  repo_name = repo_url.split(\"/\")[-1]\n",
        "  repo_path = f\"/content/{repo_name}\"\n",
        "  Repo.clone_from(repo_url, repo_path)\n",
        "  return str(repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_1zslPsDmJQ"
      },
      "outputs": [],
      "source": [
        "repo_url = \"https://github.com/CoderAgent/SecureAgent\"\n",
        "path = clone_repo(repo_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaEGmODkxhJL"
      },
      "outputs": [],
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h', '.md'}\n",
        "\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQs2cJyNza30"
      },
      "source": [
        "# Get file content using relative path from repo root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vs5GvY02G_z"
      },
      "outputs": [],
      "source": [
        "def get_function_py(content):\n",
        "  tree = ast.parse(content)\n",
        "\n",
        "  functions = []\n",
        "\n",
        "  for node in ast.walk(tree):\n",
        "    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
        "      header = f\"def {node.name}(\"\n",
        "      header += \", \".join(arg.arg for arg in node.args.args)\n",
        "      header += \"):\"\n",
        "\n",
        "      body = ast.get_source_segment(content, node)\n",
        "\n",
        "      functions.append({\"header\": header, \"body\": body})\n",
        "\n",
        "  return functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulO-P6Pnybb7"
      },
      "outputs": [],
      "source": [
        "def get_file_content(file_path, repo_path, file_extension):\n",
        "  try:\n",
        "    contents = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "      content = f.read()\n",
        "\n",
        "      rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "      if file_extension == \".py\":\n",
        "        functions = get_function_py(content)\n",
        "      else:\n",
        "        return {\n",
        "            \"name\": rel_path,\n",
        "            \"function\": None,\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "      for item in functions:\n",
        "        func_header, func_body = item[\"header\"], item[\"body\"]\n",
        "        contents.append({\"name\": rel_path, \"function\": func_header, \"content\": func_body})\n",
        "\n",
        "      return contents\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error reading file {file_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "  \"\"\"\n",
        "  Get content of supported code files from local repository.\n",
        "\n",
        "  Args:\n",
        "    repo_path: Path to local repo\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file names and contents\n",
        "  \"\"\"\n",
        "\n",
        "  files_content = []\n",
        "\n",
        "  try:\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "      # Skip if current directory is in ignored directories\n",
        "\n",
        "      if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "        continue\n",
        "\n",
        "      # Process each file in current directory\n",
        "      for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        file_extension = os.path.splitext(file)[1]\n",
        "\n",
        "        if file_extension in SUPPORTED_EXTENSIONS:\n",
        "          file_content = get_file_content(file_path, repo_path, file_extension)\n",
        "\n",
        "          if file_content:\n",
        "            if isinstance(file_content, dict):\n",
        "              files_content.append(file_content)\n",
        "\n",
        "            else:\n",
        "              for content in file_content:\n",
        "                files_content.append(content)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error reading repository: {str(e)}\")\n",
        "\n",
        "  return files_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlu7TrhU0vyy"
      },
      "outputs": [],
      "source": [
        "file_content = get_main_files_content(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJy5tUkLwWlC"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cAYCLC6wYaw"
      },
      "outputs": [],
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4qOtfH4wlRK"
      },
      "outputs": [],
      "source": [
        "text = \"I am a software developer\"\n",
        "\n",
        "embeddings = get_huggingface_embeddings(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umKbNfk3aBOL"
      },
      "source": [
        "# Using Pinecone\n",
        "**1. Create an account on [Pinecone.io](https://app.pinecone.io/)**\n",
        "\n",
        "**2. Create a new index called \"codebase-rag\" and set the dimensions to 768. Leave the rest of the settings as they are.**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 58 50 PM](https://github.com/user-attachments/assets/f5fda046-4087-432a-a8c2-86e061005238)\n",
        "\n",
        "\n",
        "\n",
        "**3. Create an API Key for Pinecone**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)\n",
        "\n",
        "\n",
        "**4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL9EU4I41mMW"
      },
      "outputs": [],
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I0qX_sG3QTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2690df62-6e92-4dfc-cca8-c44fe629a886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-5982ffb8f713>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n",
            "<ipython-input-20-5982ffb8f713>:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n"
          ]
        }
      ],
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3YwOwpG3SVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb9baf9-d641-49dd-b946-c0fa98bced78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-f40ee58eb49e>:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings(),\n"
          ]
        }
      ],
      "source": [
        "documents = []\n",
        "\n",
        "for file in file_content:\n",
        "    if file['function']:\n",
        "      doc = Document(\n",
        "          page_content=f\"{file['name']}\\n{file['content']}\",\n",
        "          metadata={\"source\": file['name'], \"function\": file['function']}\n",
        "      )\n",
        "    else:\n",
        "      doc = Document(\n",
        "        page_content=f\"{file['name']}\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "    )\n",
        "\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    namespace=repo_url\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75xrBVCrRL6"
      },
      "source": [
        "# Perform RAG\n",
        "\n",
        "1. Get your Groq API Key [here](https://console.groq.com/keys)\n",
        "\n",
        "2. Paste your Groq API Key into your Google Colab secrets, and make sure to enable permissions for it\n",
        "\n",
        "![Screenshot 2024-11-25 at 12 00 16 AM](https://github.com/user-attachments/assets/e5525d29-bca6-4dbd-892b-cc770a6b281d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIkdaRApGqYv"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWxQbWmtG_DI"
      },
      "outputs": [],
      "source": [
        "query = \"How are python files parsed?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8GoGqxsHEwh"
      },
      "outputs": [],
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqF3Wv6GHJMm"
      },
      "outputs": [],
      "source": [
        "# Feel free to change the \"top_k\" parameter to be a higher or lower number\n",
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=repo_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv_eVii9HLoR"
      },
      "outputs": [],
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja-jnGhTHaYQ"
      },
      "outputs": [],
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e1G3f6FHiKz"
      },
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript, Python, Java, C++, Go, Rust, C, and Swift.\n",
        "\n",
        "Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "\"\"\"\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBi56NBnjYMs"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIFj_rnAHm-Y"
      },
      "outputs": [],
      "source": [
        "def perform_rag(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=repo_url)\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    # Modify the prompt below as need to improve the response quality\n",
        "    system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript, Python, Java, C++, Go, Rust, C, and Swift.\n",
        "\n",
        "    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return llm_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dze5WMvHsqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959cc587-ed38-4d3b-e4f8-a14c09f1b367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The JavascriptParser is used in two main places in the provided codebase:\n",
            "\n",
            "1. **diffContextPerHunk**: In this function, which is part of `src/context/review.ts`, `JavascriptParser` is used to determine the `enclosingContext` for each hunk in the diff. The `findEnclosingContext` method of `JavascriptParser` is called to identify the enclosing function for a given range of lines. This enclosing function is then used to construct the context string for the hunk.\n",
            "2. **getParserForExtension**: In this function, which is part of `src/constants.ts`, `JavascriptParser` is one of the parsers registered for specific file extensions (javascript, typescript, jsx, and tsx). This function returns the parser instance based on the file extension of a given file. If the file has a .js or .ts extension, `JavascriptParser` will be used.\n",
            "\n",
            "Additionally, the `JavascriptParser` is used by the `functionContextPatchStrategy` function, which in turn uses `diffContextPerHunk` to generate a patch strategy for reviewing code changes.\n",
            "\n",
            "The `dryRun` method of `JavascriptParser` is also an empty function that probably needs to be implemented later to perform some kind of dry run or validation for the code. However, it is not currently being used.\n",
            "\n",
            "The `EXTENSIONS_TO_PARSERS` map in `src/constants.ts` maps the file extensions to their corresponding parsers, which includes `JavascriptParser` for `.js`, `.jsx`, `.ts`, and `.tsx` extensions. This suggests that the codebase is designed to support multiple programming languages and their respective parsers.\n",
            "\n",
            "The main use case for `JavascriptParser` is to parse and analyze JavaScript code to determine the context (e.g., enclosing functions) for code changes, which is then used to generate a review strategy.\n"
          ]
        }
      ],
      "source": [
        "response = perform_rag(\"How is the javascript parser used?\")\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtkaGleLGzpkFcV5evorPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}